{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Network Traffic Anomaly Detection - Offline Training and Evaluation\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This notebook demonstrates the offline training and evaluation of machine learning models for network traffic anomaly detection. It leverages the feature engineering concepts from `src/features.py` and model architectures discussed in the original `NW_sec_package (1).ipynb`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import joblib\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MinMaxScaler, OneHotEncoder, LabelEncoder\n",
        "from sklearn.ensemble import IsolationForest\n",
        "from sklearn.metrics import classification_report, confusion_matrix, balanced_accuracy_score\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import os\n",
        "\n",
        "# Ensure reproducibility\n",
        "tf.random.set_seed(42)\n",
        "np.random.seed(42)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Data Loading and Preparation\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "For this demonstration, we'll use a synthetic dataset or assume the UNSW-NB15 dataset (or similar pre-processed CSVs) are available in a `dataset/` subdirectory. In a real-world scenario, you would download and prepare your training data here.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Synthetic data generation for demonstration if real dataset is not available\n",
        "def generate_synthetic_data(num_samples=1000):\n",
        "    data = {\n",
        "        'dur': np.random.uniform(0.0, 60.0, num_samples),\n",
        "        'proto': np.random.choice(['tcp', 'udp', 'icmp'], num_samples),\n",
        "        'service': np.random.choice(['-', 'http', 'dns', 'ftp'], num_samples),\n",
        "        'state': np.random.choice(['FIN', 'INT', 'REQ'], num_samples),\n",
        "        'spkts': np.random.randint(1, 100, num_samples),\n",
        "        'dpkts': np.random.randint(0, 100, num_samples),\n",
        "        'sbytes': np.random.randint(100, 10000, num_samples),\n",
        "        'dbytes': np.random.randint(0, 10000, num_samples),\n",
        "        'rate': np.random.uniform(0.0, 1000.0, num_samples),\n",
        "        'sload': np.random.uniform(1000.0, 1000000.0, num_samples),\n",
        "        'dload': np.random.uniform(0.0, 1000000.0, num_samples),\n",
        "        'synack': np.random.uniform(0.0, 0.1, num_samples),\n",
        "        'ackdat': np.random.uniform(0.0, 0.1, num_samples),\n",
        "        'ct_srv_src': np.random.randint(1, 20, num_samples),\n",
        "        'ct_state_ttl': np.random.randint(0, 255, num_samples),\n",
        "        'ct_dst_ltm': np.random.randint(1, 20, num_samples),\n",
        "        'ct_src_dport_ltm': np.random.randint(1, 10, num_samples),\n",
        "        'ct_dst_sport_ltm': np.random.randint(1, 10, num_samples),\n",
        "        'ct_dst_src_ltm': np.random.randint(1, 20, num_samples),\n",
        "        'ct_src_ltm': np.random.randint(1, 20, num_samples),\n",
        "        'ct_srv_dst': np.random.randint(1, 20, num_samples),\n",
        "        'is_sm_ips_ports': np.random.choice([0, 1], num_samples),\n",
        "        'label': np.random.choice([0, 1, 2], num_samples, p=[0.7, 0.15, 0.15]) # 0: Normal, 1: DoS, 2: Reconnaissance\n",
        "    }\n",
        "    df = pd.DataFrame(data)\n",
        "    \n",
        "    # Introduce some anomalies for specific labels\n",
        "    df.loc[df['label'] == 1, 'sload'] = np.random.uniform(1e6, 1e7, df[df['label'] == 1].shape[0]) # High sload for DoS\n",
        "    df.loc[df['label'] == 2, 'ct_dst_sport_ltm'] = np.random.randint(5, 15, df[df['label'] == 2].shape[0]) # High distinct ports for Reconnaissance\n",
        "\n",
        "    # Generate additional columns as if from `src/features.py` derived features\n",
        "    df[\"pkt_ratio\"] = df[\"spkts\"] / (df[\"dpkts\"] + 1)\n",
        "    df[\"byte_ratio\"] = df[\"sbytes\"] / (df[\"dbytes\"] + 1)\n",
        "    df[\"syn_rate\"] = df[\"synack\"] / (df[\"spkts\"] + 1)\n",
        "    df[\"ack_rate\"] = df[\"ackdat\"] / (df[\"spkts\"] + 1)\n",
        "    df[\"avg_pkt_size\"] = df[\"sbytes\"] / (df[\"spkts\"] + 1)\n",
        "    df['bytes_per_pkt'] = (df['sbytes'] + df['dbytes']) / (df['spkts'] + df['dpkts'] + 1)\n",
        "    df['pkt_size_ratio'] = df['sbytes'] / (df['dbytes'] + 1)\n",
        "    df['port_diversity'] = (df['ct_src_dport_ltm'] + df['ct_dst_sport_ltm']) / (df['ct_srv_dst'] + 1)\n",
        "\n",
        "    return df\n",
        "\n",
        "# Load dataset (or generate synthetic data)\n",
        "try:\n",
        "    # Adjust path as necessary if you have the actual UNSW-NB15 dataset CSVs\n",
        "    train_df = pd.read_csv(\"dataset/UNSW_NB15_training-set.csv\")\n",
        "    test_df = pd.read_csv(\"dataset/UNSW_NB15_testing-set.csv\")\n",
        "    print(\"Loaded UNSW-NB15 dataset.\")\n",
        "except FileNotFoundError:\n",
        "    print(\"UNSW-NB15 dataset not found. Generating synthetic data...\")\n",
        "    train_df = generate_synthetic_data(num_samples=10000)\n",
        "    test_df = generate_synthetic_data(num_samples=2000)\n",
        "    # Map numerical labels back to original attack_cat for consistency with original notebook\n",
        "    label_map = {0: 'Normal', 1: 'DoS', 2: 'Reconnaissance'}\n",
        "    train_df['attack_cat'] = train_df['label'].map(label_map)\n",
        "    test_df['attack_cat'] = test_df['label'].map(label_map)\n",
        "\n",
        "\n",
        "print(f\"Train data shape: {train_df.shape}\")\n",
        "print(f\"Test data shape: {test_df.shape}\")\n",
        "print(\"Train head:\\n\", train_df.head())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Feature Engineering and Preprocessing\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We select features based on their relevance as identified in the original notebook and `src/features.py`. We then apply Min-Max Scaling to numerical features and One-Hot Encoding to categorical features.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "selected_features = [\n",
        "    'dur', 'spkts', 'dpkts', 'sbytes', 'dbytes', 'rate', 'sload', 'dload',\n",
        "    'synack', 'ackdat', 'state',\n",
        "    'ct_srv_src', 'ct_state_ttl', 'ct_dst_ltm', 'ct_src_dport_ltm',\n",
        "    'ct_dst_sport_ltm', 'ct_dst_src_ltm', 'ct_src_ltm', 'ct_srv_dst',\n",
        "    'proto', 'service',\n",
        "    'pkt_ratio', 'byte_ratio', 'syn_rate', 'ack_rate', 'avg_pkt_size',\n",
        "    'bytes_per_pkt', 'pkt_size_ratio', 'port_diversity'\n",
        "]\n",
        "\n",
        "# Ensure all selected features exist in both train and test data\n",
        "for feature in selected_features:\n",
        "    if feature not in train_df.columns:\n",
        "        train_df[feature] = 0 # Default value\n",
        "    if feature not in test_df.columns:\n",
        "        test_df[feature] = 0 # Default value\n",
        "\n",
        "# Define target column\n",
        "target_column = 'attack_cat' # Or 'label' if using synthetic numeric labels\n",
        "\n",
        "X_train = train_df[selected_features]\n",
        "y_train = train_df[target_column]\n",
        "X_test = test_df[selected_features]\n",
        "y_test = test_df[target_column]\n",
        "\n",
        "# Identify numerical and categorical columns\n",
        "numeric_cols = X_train.select_dtypes(include=np.number).columns.tolist()\n",
        "categorical_cols = X_train.select_dtypes(include='object').columns.tolist()\n",
        "\n",
        "print(f\"Numerical columns: {numeric_cols}\")\n",
        "print(f\"Categorical columns: {categorical_cols}\")\n",
        "\n",
        "# Initialize preprocessors\n",
        "scaler = MinMaxScaler()\n",
        "encoder = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
        "label_encoder = LabelEncoder()\n",
        "\n",
        "# Scale numerical features\n",
        "X_train_scaled = scaler.fit_transform(X_train[numeric_cols])\n",
        "X_test_scaled = scaler.transform(X_test[numeric_cols])\n",
        "\n",
        "X_train[numeric_cols] = X_train_scaled\n",
        "X_test[numeric_cols] = X_test_scaled\n",
        "\n",
        "# Encode categorical features\n",
        "X_train_encoded = encoder.fit_transform(X_train[categorical_cols])\n",
        "X_test_encoded = encoder.transform(X_test[categorical_cols])\n",
        "\n",
        "# Get feature names for encoded categorical features\n",
        "encoded_feature_names = encoder.get_feature_names_out(categorical_cols)\n",
        "\n",
        "# Combine scaled numerical and encoded categorical features\n",
        "X_train_final = pd.concat([X_train[numeric_cols].reset_index(drop=True), \n",
        "                           pd.DataFrame(X_train_encoded, columns=encoded_feature_names)], axis=1)\n",
        "X_test_final = pd.concat([X_test[numeric_cols].reset_index(drop=True), \n",
        "                          pd.DataFrame(X_test_encoded, columns=encoded_feature_names)], axis=1)\n",
        "\n",
        "# Encode target labels\n",
        "y_train_encoded = label_encoder.fit_transform(y_train)\n",
        "y_test_encoded = label_encoder.transform(y_test)\n",
        "\n",
        "print(f\"Final X_train shape: {X_train_final.shape}\")\n",
        "print(f\"Final X_test shape: {X_test_final.shape}\")\n",
        "print(f\"Encoded target classes: {list(label_encoder.classes_)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Model Training and Evaluation\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.1 Isolation Forest\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\n--- Training Isolation Forest Model ---\")\n",
        "iso_forest = IsolationForest(random_state=42, contamination=0.05) # Adjust contamination as needed\n",
        "iso_forest.fit(X_train_final)\n",
        "\n",
        "# Predict anomaly scores (-1 for anomaly, 1 for normal)\n",
        "isol_predictions = iso_forest.predict(X_test_final)\n",
        "\n",
        "# Convert to binary labels for evaluation (e.g., -1 -> anomaly, 1 -> normal)\n",
        "y_pred_iso = np.array([1 if p == -1 else 0 for p in isol_predictions])\n",
        "y_true_iso = np.array([1 if l != 'Normal' else 0 for l in y_test]) # Assuming 'Normal' is the non-anomaly label\n",
        "\n",
        "print(\"Isolation Forest Evaluation:\")\n",
        "print(f\"Balanced Accuracy: {balanced_accuracy_score(y_true_iso, y_pred_iso):.4f}\")\n",
        "print(\"Classification Report:\\n\", classification_report(y_true_iso, y_pred_iso))\n",
        "\n",
        "# Confusion Matrix\n",
        "cm_iso = confusion_matrix(y_true_iso, y_pred_iso)\n",
        "plt.figure(figsize=(6,4))\n",
        "sns.heatmap(cm_iso, annot=True, fmt='d', cmap='Blues', xticklabels=['Normal', 'Anomaly'], yticklabels=['Normal', 'Anomaly'])\n",
        "plt.title('Isolation Forest Confusion Matrix')\n",
        "plt.xlabel('Predicted'); plt.ylabel('True')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.2 LSTM Model (from original notebook)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\n--- Training LSTM Model ---\")\n",
        "\n",
        "# Reshape data for LSTM (samples, timesteps, features)\n",
        "n_features = X_train_final.shape[1]\n",
        "timesteps = 1  # For a single-step input\n",
        "\n",
        "X_train_lstm = np.array(X_train_final, dtype=np.float32).reshape((-1, timesteps, n_features))\n",
        "X_test_lstm = np.array(X_test_final, dtype=np.float32).reshape((-1, timesteps, n_features))\n",
        "\n",
        "num_classes = len(label_encoder.classes_)\n",
        "\n",
        "# Define the LSTM model architecture (from original notebook)\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.LSTM(64, input_shape=(timesteps, n_features)),\n",
        "    tf.keras.layers.Dropout(0.3),\n",
        "    tf.keras.layers.Dense(64, activation='relu'),\n",
        "    tf.keras.layers.Dense(num_classes, activation='softmax')\n",
        "])\n",
        "\n",
        "model.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),\n",
        "    loss='sparse_categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "# Callbacks\n",
        "early_stop = tf.keras.callbacks.EarlyStopping(\n",
        "    monitor='val_loss', patience=6, restore_best_weights=True\n",
        ")\n",
        "reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(\n",
        "    monitor='val_loss', factor=0.5, patience=3, min_lr=1e-5\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(\n",
        "    X_train_lstm, y_train_encoded,\n",
        "    validation_split=0.2,\n",
        "    epochs=50,\n",
        "    batch_size=128,\n",
        "    callbacks=[early_stop, reduce_lr],\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Evaluate LSTM Model\n",
        "print(\"\\nLSTM Model Evaluation:\")\n",
        "loss, accuracy = model.evaluate(X_test_lstm, y_test_encoded, verbose=0)\n",
        "print(f\"Test Loss: {loss:.4f}, Test Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "# Predictions\n",
        "predictions_lstm = np.argmax(model.predict(X_test_lstm), axis=1)\n",
        "y_pred_lstm_labels = label_encoder.inverse_transform(predictions_lstm)\n",
        "y_true_lstm_labels = label_encoder.inverse_transform(y_test_encoded)\n",
        "\n",
        "print(\"Classification Report (LSTM):\\n\", classification_report(y_true_lstm_labels, y_pred_lstm_labels))\n",
        "\n",
        "# Confusion Matrix\n",
        "cm_lstm = confusion_matrix(y_true_lstm_labels, y_pred_lstm_labels)\n",
        "plt.figure(figsize=(7,5))\n",
        "sns.heatmap(cm_lstm, annot=True, fmt='d', cmap='Blues', xticklabels=label_encoder.classes_, yticklabels=label_encoder.classes_)\n",
        "plt.title('LSTM Confusion Matrix')\n",
        "plt.xlabel('Predicted'); plt.ylabel('True')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Save the trained models and preprocessors to the `data/models/` directory for use in the real-time detection system.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "models_dir = \"data/models\"\n",
        "os.makedirs(models_dir, exist_ok=True)\n",
        "\n",
        "# Save Isolation Forest model and its preprocessors\n",
        "joblib.dump(iso_forest, os.path.join(models_dir, \"IsolationForest.pkl\"))\n",
        "joblib.dump(scaler, os.path.join(models_dir, \"IsolationForest_scaler.pkl\")) # Assuming same scaler for both models for now\n",
        "joblib.dump(encoder, os.path.join(models_dir, \"IsolationForest_encoder.pkl\"))\n",
        "joblib.dump(label_encoder, os.path.join(models_dir, \"IsolationForest_label_encoder.pkl\"))\n",
        "print(\"Saved IsolationForest model and preprocessors.\")\n",
        "\n",
        "# Save LSTM model and its preprocessors\n",
        "model.save(os.path.join(models_dir, \"lstm_model.h5\"))\n",
        "joblib.dump(scaler, os.path.join(models_dir, \"LSTM_scaler.pkl\")) # Assuming same scaler for both models for now\n",
        "joblib.dump(encoder, os.path.join(models_dir, \"LSTM_encoder.pkl\"))\n",
        "joblib.dump(label_encoder, os.path.join(models_dir, \"LSTM_label_encoder.pkl\"))\n",
        "print(\"Saved LSTM model and preprocessors.\")\n",
        "\n",
        "print(\"All models and preprocessors saved to data/models/\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.13.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
